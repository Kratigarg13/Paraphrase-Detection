{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "paraphrase_data_preprocessing.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ObXcu-3mDGn"
      },
      "source": [
        "#  Paraphrase Detection Data Preprocessing\n",
        "MSRP Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDhTB5UmmbAB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a0bc5d4-0e2e-4a2a-f5be-f1f15cd5c7e6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCrYAVZNmDG8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44e036ca-904c-4cbb-8fbe-b20d5fb4e181"
      },
      "source": [
        "''' This script is to preprocess data from the MSRP dataset for paraphrase detection\n",
        "Adapted from Keras example at https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py\n",
        "'''\n",
        "import os\n",
        "import numpy as np\n",
        "import datetime, time, json\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "!pip install simplejson as json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting simplejson\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/04/377418ac1e530ce2a196b54c6552c018fdf1fe776718053efb1f216bffcd/simplejson-3.17.2-cp37-cp37m-manylinux2010_x86_64.whl (128kB)\n",
            "\r\u001b[K     |██▌                             | 10kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 20kB 16.7MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 30kB 10.9MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 40kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 51kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 61kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 71kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 81kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 92kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 102kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 112kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 122kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 5.8MB/s \n",
            "\u001b[?25hCollecting as\n",
            "  Downloading https://files.pythonhosted.org/packages/b4/08/226c133ec497d25a63edb38527c02db093c7d89e6d4cdc91078834486a5d/as-0.1-py3-none-any.whl\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement json (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for json\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNONlSU6mDHB"
      },
      "source": [
        "t0 = time.time()\n",
        "BASE_DIR = ''\n",
        "GLOVE_DIR = os.path.join(BASE_DIR, '/content/drive/MyDrive/ParaphraseDetection/')\n",
        "MSR_DIR = os.path.join(BASE_DIR, '/content/drive/MyDrive/ParaphraseDetection/')\n",
        "MSR_FILE = 'msr_paraphrase_train_test.txt'\n",
        "GLOVE_FILE = 'glove.6B.200d.txt'\n",
        "MAX_SEQUENCE_LENGTH = 30\n",
        "MAX_NUM_WORDS = 20000\n",
        "EMBEDDING_DIM = 200\n",
        "VALIDATION_SPLIT = 0.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UH6t_NyjmDHH"
      },
      "source": [
        "# Extract sentence pairs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X69rWcHamDHI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f391c4f-408e-472c-85cb-f9668276be9b"
      },
      "source": [
        "# Process sentence pairs from MSRP corpus\n",
        "\n",
        "print(\"Processing\", MSR_FILE)\n",
        "\n",
        "sentence1 = []\n",
        "sentence2 = []\n",
        "label = []\n",
        "\n",
        "with open(MSR_DIR + MSR_FILE, 'r', encoding='utf8') as f:\n",
        "    f.readline()  # skipping the header of the file\n",
        "    for line in f:\n",
        "        text = line.strip().split('\\t')\n",
        "        sentence1.append(text[3])\n",
        "        sentence2.append(text[4])\n",
        "        label.append(int(text[0]))\n",
        "        \n",
        "print ('Sentence pairs: %d' % len(sentence1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing msr_paraphrase_train_test.txt\n",
            "Sentence pairs: 5800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO_OK12PmDHL"
      },
      "source": [
        "# Build tokenized word index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT0cTqajmDHM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "885b12ec-b252-4933-b177-dd6287b24c60"
      },
      "source": [
        "# Build tokenized word index\n",
        "\n",
        "sentences = sentence1 + sentence2\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sentence1_word_sequences = tokenizer.texts_to_sequences(sentence1)\n",
        "sentence2_word_sequences = tokenizer.texts_to_sequences(sentence2)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "print(\"Words in index: %d\" % len(word_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words in index: 16537\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MndK7Sr8mDHN"
      },
      "source": [
        "# Download and process GloVe embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugVzVeuemDHP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ae1fb0a-9e07-4c19-f71b-228bf25b4889"
      },
      "source": [
        "\n",
        "print(\"Processing\", GLOVE_FILE)\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(os.path.join(GLOVE_DIR, GLOVE_FILE), encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split(' ')\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Indexing word vectors.\n",
            "Processing glove.6B.200d.txt\n",
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITfgplEumDHR"
      },
      "source": [
        "# Prepare word embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYxQwAD0mDHS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6232b6ff-2ea1-4e3f-a6e2-5b8fd7bbdbb5"
      },
      "source": [
        "print('Preparing embedding matrix.')\n",
        "\n",
        "num_words = min(MAX_NUM_WORDS, len(word_index))\n",
        "word_embedding_matrix = np.zeros((num_words + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if i > MAX_NUM_WORDS:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        word_embedding_matrix[i] = embedding_vector\n",
        "\n",
        "print('Null word embeddings: %d' % np.sum(np.sum(word_embedding_matrix, axis=1) == 0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing embedding matrix.\n",
            "Null word embeddings: 1483\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUSZduQymDHT"
      },
      "source": [
        "# Prepare training data tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqDrTzx5mDHU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "814c0446-b1cb-434c-f399-84abfdbef6d2"
      },
      "source": [
        "s1_data = pad_sequences(sentence1_word_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "s2_data = pad_sequences(sentence2_word_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "labels = np.array(label, dtype=int)\n",
        "print('Shape of sentence1 data tensor:', s1_data.shape)\n",
        "print('Shape of label tensor:', labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of sentence1 data tensor: (5800, 30)\n",
            "Shape of label tensor: (5800,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M56585aLmDHW"
      },
      "source": [
        "# Save data to files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BPXucERmDHX"
      },
      "source": [
        "S1_TRAINING_DATA_FILE = '/content/drive/MyDrive/ParaphraseDetection/paraphrase4/s1_train.npy'\n",
        "S2_TRAINING_DATA_FILE = '/content/drive/MyDrive/ParaphraseDetection/paraphrase4/s2_train.npy'\n",
        "LABEL_TRAINING_DATA_FILE = '/content/drive/MyDrive/ParaphraseDetection/paraphrase4/label_train.npy'\n",
        "WORD_EMBEDDING_MATRIX_FILE = '/content/drive/MyDrive/ParaphraseDetection/paraphrase4/word_embedding_matrix.npy'\n",
        "NUM_WORDS_DATA_FILE = '/content/drive/MyDrive/ParaphraseDetection/paraphrase4/num_words.json'\n",
        "\n",
        "np.save(open(S1_TRAINING_DATA_FILE, 'wb'), s1_data)\n",
        "np.save(open(S2_TRAINING_DATA_FILE, 'wb'), s2_data)\n",
        "np.save(open(LABEL_TRAINING_DATA_FILE, 'wb'), labels)\n",
        "np.save(open(WORD_EMBEDDING_MATRIX_FILE, 'wb'), word_embedding_matrix)\n",
        "with open(NUM_WORDS_DATA_FILE, 'w') as f:\n",
        "    json.dump({'num_words': num_words}, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQvkfE1MmDHY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c415332-77b9-4a0a-a53c-c6acb32a914f"
      },
      "source": [
        "print(type(s1_data))\n",
        "print(s1_data[0].shape)\n",
        "print(s2_data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(30,)\n",
            "(5800, 30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw6z52PhmDHZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a44fd914-baa8-42f5-9452-a5eade66fba1"
      },
      "source": [
        "t1 = time.time()\n",
        "print(\"Preprocessing ended at\", datetime.datetime.now())\n",
        "print(\"Minutes elapsed: %f\" % ((t1 - t0) / 60.))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preprocessing ended at 2021-04-12 08:23:04.175777\n",
            "Minutes elapsed: 22.460537\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOGceLMuLU9q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}