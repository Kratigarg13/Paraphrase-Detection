{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "paraphrase2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-i2Qf99XIiU"
      },
      "source": [
        "import os\n",
        "import nltk\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "def load_dataset(dataset_path_train,word2idx):\n",
        "\n",
        "    with open(dataset_path_train,'r') as file1:\n",
        "      \n",
        "      sentence_one = []\n",
        "      sentence_two = []\n",
        "      y_true = []\n",
        "\n",
        "      max1 = 0\n",
        "      max2 = 0\n",
        "\n",
        "      for index, line in enumerate(file1):\n",
        "        \n",
        "        if index == 0:\n",
        "          continue\n",
        "\n",
        "        s_1 = []\n",
        "        s_2 = []\n",
        "\n",
        "        values = line.split(\"\\t\")\n",
        "        \n",
        "        #Sentence 1\n",
        "        \n",
        "        words = nltk.word_tokenize(values[3])\n",
        "        words = [word.lower() for word in words]\n",
        "\n",
        "        s_1.extend([word2idx.get(word,word2idx['UNK']) for word in words])\n",
        "\n",
        "        #Sentence 2\n",
        "        # words = values[4].split(\" \")\n",
        "        words = nltk.word_tokenize(values[4])\n",
        "        words = [word.lower() for word in words]\n",
        "\n",
        "        s_2.extend([word2idx.get(word,word2idx['UNK']) for word in words])\n",
        "        \n",
        "\n",
        "        if len(s_1) > max1:\n",
        "          max1 = len(s_1)\n",
        "\n",
        "        if len(s_2) > max2:\n",
        "          max2 = len(s_2)\n",
        "\n",
        "        y_true.append(np.asarray(values[0]))\n",
        "\n",
        "        sentence_one.append(np.pad(s_1,(0,41-len(s_1)),'constant',constant_values=(0)))\n",
        "        sentence_two.append(np.pad(s_2,(0,41-len(s_2)),'constant',constant_values=(0)))\n",
        "\n",
        "\n",
        "        # self.sentence_one.append(np.asarray(s_1[0:self.sen_len]))\n",
        "        # self.sentence_two.append(np.asarray(s_2[0:self.sen_len]))\n",
        "\n",
        "    sentence_one = np.stack(sentence_one)\n",
        "    sentence_two = np.stack(sentence_two)\n",
        "    y_true = np.stack(y_true)\n",
        "\n",
        "    #print self.weights\n",
        "\n",
        "    print(\"Max_train:\",max1,max2)\n",
        "\n",
        "    print(sentence_one.shape,sentence_two.shape,y_true.shape)\n",
        "    \n",
        "    \n",
        "\n",
        "    return sentence_one,sentence_two,y_true\n",
        "\n",
        "\n",
        "def load_dataset_test(dataset_path_test,word2idx):\n",
        "\n",
        "    with open(dataset_path_test,'r') as file1:\n",
        "      \n",
        "      sentence_one_test = []\n",
        "      sentence_two_test = []\n",
        "      y_true_test = []\n",
        "\n",
        "      max1 = 0\n",
        "      max2 = 0\n",
        "\n",
        "      for index, line in enumerate(file1):\n",
        "        \n",
        "        if index == 0:\n",
        "          continue\n",
        "\n",
        "        s_1 = []\n",
        "        s_2 = []\n",
        "\n",
        "        values = line.split(\"\\t\")\n",
        "        \n",
        "        #Sentence 1\n",
        "        # words = values[3].split(\" \")\n",
        "        words = nltk.word_tokenize(values[3])\n",
        "        words = [word.lower() for word in words]\n",
        "\n",
        "        s_1.extend([word2idx.get(word,word2idx['UNK']) for word in words])\n",
        "\n",
        "        #Sentence 2\n",
        "        # words = values[4].split(\" \")\n",
        "        words = nltk.word_tokenize(values[4])\n",
        "        words = [word.lower() for word in words]\n",
        "\n",
        "        s_2.extend([word2idx.get(word,word2idx['UNK']) for word in words])\n",
        "        \n",
        "        if len(s_1) > max1:\n",
        "          max1 = len(s_1)\n",
        "\n",
        "        if len(s_2) > max2:\n",
        "          max2 = len(s_2)\n",
        "\n",
        "\n",
        "        y_true_test.append(np.asarray(values[0]))\n",
        "\n",
        "        sentence_one_test.append(np.pad(s_1,(0,41-len(s_1)),'constant',constant_values=(0)))\n",
        "        sentence_two_test.append(np.pad(s_2,(0,41-len(s_2)),'constant',constant_values=(0)))\n",
        "\n",
        "        \n",
        "\n",
        "    print(\"Max_test:\",max1,max2)\n",
        "\n",
        "    sentence_one_test = np.stack(sentence_one_test)\n",
        "    sentence_two_test = np.stack(sentence_two_test)\n",
        "    y_true_test = np.stack(y_true_test)\n",
        "\n",
        "    \n",
        "    print(sentence_one_test.shape,sentence_two_test.shape,y_true_test.shape)\n",
        "\n",
        "    return sentence_one_test,sentence_two_test,y_true_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WelurOy4uf9H"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "eps = 1e-6\n",
        "\n",
        "def cosine_sim(x1,x2):\n",
        "\n",
        "    # x1 shape : (batch_size, 1, sequence_length, rnn_hidden_size)\n",
        "    # x2 shape : (batch_size, sequence_length, 1, rnn_hidden_size)\n",
        "\n",
        "    # Shape : (batch_size, sequence_length, sequence_length)\n",
        "    numerator = tf.reduce_sum(tf.multiply(x1,x2),axis=-1)\n",
        "\n",
        "    # Shape : (batch_size, 1, sequence_length)\n",
        "    x1_norm = tf.sqrt(tf.maximum(tf.reduce_sum(tf.square(x1),axis=-1),eps))\n",
        "\n",
        "    # Shape : (batch_size, sequence_length, 1)\n",
        "    x2_norm = tf.sqrt(tf.maximum(tf.reduce_sum(tf.square(x2),axis=-1),eps))\n",
        "\n",
        "    return numerator / (x1_norm * x2_norm)\n",
        "\n",
        "\n",
        "def cosine_matrix(x1,x2):\n",
        "\n",
        "    # x1 shape : (batch_size, sequence_length, rnn_hidden_size)\n",
        "    # x2 shape : (batch_size, sequence_length, rnn_hidden_size)\n",
        "\n",
        "    # Shape : (batch_size, 1, sequence_length, rnn_hidden_size)\n",
        "    x1_exp = tf.expand_dims(x1,1)\n",
        "\n",
        "    # Shape : (batch_size, sequence_length, 1, rnn_hidden_size)\n",
        "    x2_exp = tf.expand_dims(x2,2)\n",
        "\n",
        "    # Shape : (batch_size, sequence_length, sequence_length, rnn_hidden_size)\n",
        "    cosine_sim_matrix = cosine_sim(x1_exp,x2_exp)\n",
        "\n",
        "    return cosine_sim_matrix\n",
        "\n",
        "def get_last_output(output):\n",
        "\n",
        "    #print tf.shape(output)[1]\n",
        "    \n",
        "    output = tf.transpose(output, [1, 0, 2])\n",
        "    last = tf.gather(output, int(output.get_shape()[0]) - 1)\n",
        "    \n",
        "    #last = tf.gather(output, int(tf.shape(output)[0]) - 1)\n",
        "\n",
        "    # Shape : (batch_size, 1, rnn_hidden_size) \n",
        "    return last\n",
        "\n",
        "\n",
        "def expand_1D(t,weights):\n",
        "\n",
        "    # Shape : (1, rnn_hidden_size)\n",
        "    t_exp = tf.expand_dims(t,axis=0)\n",
        "\n",
        "    # Shape : (perspectives, rnn_hidden_size)\n",
        "    return tf.multiply(t_exp,weights)\n",
        "\n",
        "def expand_2D(t,weights):\n",
        "\n",
        "    # Shape : (sequence_length, 1, rnn_hidden_size)\n",
        "    t_exp = tf.expand_dims(t,axis=1)\n",
        "\n",
        "    # Shape : (1, perspectives, rnn_hidden_size)\n",
        "    weights_exp = tf.expand_dims(weights,axis=0)\n",
        "\n",
        "    # Shape : (sequence_length, perspectives, rnn_hidden_size)\n",
        "    return tf.multiply(t_exp,weights_exp)\n",
        "\n",
        "\n",
        "def full_matching(sentence_a_full,sentence_b_last,weights):\n",
        "\n",
        "    def single_func(input):\n",
        "\n",
        "        # Shape : (sequence_length, rnn_hidden_size)\n",
        "        sentence_a_single = input[0]\n",
        "\n",
        "        # Shape : (rnn_hidden_size)\n",
        "        sentence_b_last_single = input[1]\n",
        "\n",
        "        # Shape : (sequence_length, perspectives, rnn_hidden_size)\n",
        "        sentence_a_single = expand_2D(sentence_a_single,weights)\n",
        "\n",
        "        # Shape : (perspectives, rnn_hidden_size)\n",
        "        sentence_b_last_single = expand_1D(sentence_b_last_single,weights)\n",
        "\n",
        "        # Shape : (1, perspectives, rnn_hidden_size)\n",
        "        sentence_b_last_single = tf.expand_dims(sentence_b_last_single,0)\n",
        "\n",
        "        # Shape : (sequence_length, perspectives)\n",
        "        return cosine_sim(sentence_a_single,sentence_b_last_single)\n",
        "\n",
        "    # Shape : (batch_size, sequence_length, perspectives)\n",
        "    return tf.map_fn(single_func,(sentence_a_full,sentence_b_last),dtype=\"float\")\n",
        "\n",
        "\n",
        "\n",
        "def pooling_matching(sentence_a_full,sentence_b_full,weights):\n",
        "\n",
        "    def single_func(input):\n",
        "\n",
        "        # Shape : (sequence_length, rnn_hidden_size)\n",
        "        sentence_a_single = input[0]\n",
        "\n",
        "        # Shape : (sequence_length, rnn_hidden_size)\n",
        "        sentence_b_single = input[1]\n",
        "\n",
        "        # Shape : (sequence_length, perspectives, rnn_hidden_size)\n",
        "        sentence_a_single = expand_2D(sentence_a_single,weights)\t\t\t\n",
        "        sentence_b_single = expand_2D(sentence_b_single,weights)\n",
        "\n",
        "        # Shape : (sequence_length, 1, perspectives, rnn_hidden_size)\n",
        "        sentence_a_single = tf.expand_dims(sentence_a_single,1)\n",
        "\n",
        "        # Shape : (1, sequence_length, perspectives, rnn_hidden_size)\n",
        "        sentence_b_single = tf.expand_dims(sentence_b_single,0)\n",
        "\n",
        "        # Shape : (sequence_length, sequence_length, perspectives)\n",
        "        return cosine_sim(sentence_a_single,sentence_b_single)\n",
        "\n",
        "    # Shape : (batch_size, sequence_length, sequence_length, perspectives)\n",
        "    match_matrix = tf.map_fn(single_func,(sentence_a_full,sentence_b_full),dtype=\"float\")\n",
        "\n",
        "    # Shape : (batch_size, sequence_length, perspectives)\n",
        "    return tf.reduce_max(match_matrix,axis=2)\n",
        "\n",
        "\n",
        "def weighted_sim(sentence,sim_mat):\n",
        "\n",
        "    # Shape : (batch_size, sequence_length, sequence_length, 1)\n",
        "    sim_mat_exp = tf.expand_dims(sim_mat,-1)\n",
        "\n",
        "    # Shape : (batch_size, 1, sequence_length, rnn_hidden_size)\n",
        "    sentence = tf.expand_dims(sentence,1)\n",
        "\n",
        "    # Shape : (batch_size, sequence_length, rnn_hidden_size)\n",
        "    weighted_sim_mat = tf.reduce_sum(tf.multiply(sentence,sim_mat_exp),axis=2)\n",
        "\n",
        "    weighted_sim_mat = tf.div(weighted_sim_mat,tf.expand_dims(tf.add(tf.reduce_sum(sim_mat,axis=-1),eps),axis=-1))\n",
        "\n",
        "    return weighted_sim_mat\n",
        "\n",
        "\n",
        "def max_sim(sentence_b_full,sim_mat):\n",
        "\n",
        "    def single_func(input):\n",
        "\n",
        "        # Shape : (sequence_length, rnn_hidden_size)\n",
        "        sentence_b_single = input[0]\n",
        "\n",
        "        # Shape : (sequence_length)\n",
        "        max_index = input[1]\n",
        "\n",
        "        # Shape : (sequence_length, rnn_hidden_size)\n",
        "        return tf.gather(sentence_b_single,max_index)\n",
        "\n",
        "    max_index = tf.arg_max(sim_mat,2)\n",
        "\n",
        "    # Shape : (batch_size, sequence_length, rnn_hidden_size)\n",
        "    return tf.map_fn(single_func,(sentence_b_full,max_index),dtype=\"float\")\n",
        "\n",
        "\n",
        "def attentive_matching(sentence_a_full,weighted_sim_mat,weights):\n",
        "\n",
        "    def single_func(input):\n",
        "\n",
        "        # Shape : (sequence_length, rnn_hidden_size)\n",
        "        sentence_a_single = input[0]\n",
        "\n",
        "        # Shape : (sequence_length, rnn_hidden_size)\n",
        "        sentence_b_single_att = input[1]\n",
        "\n",
        "        # Shape : (sequence_length, perspectives, rnn_hidden_size)\n",
        "        sentence_a_single = expand_2D(sentence_a_single,weights)\n",
        "        sentence_b_single_att = expand_2D(sentence_b_single_att,weights)\n",
        "\n",
        "        # Shape : (sequence_length, perspectives)\n",
        "        return cosine_sim(sentence_a_single,sentence_b_single_att)\n",
        "\n",
        "    # Shape : (batch_size, sequence_length, perspectives)\n",
        "    return tf.map_fn(single_func,(sentence_a_full,weighted_sim_mat),dtype=\"float\")\n",
        "\n",
        "\n",
        "def match(sentence_a_fw, sentence_a_bw, sentence_b_fw, sentence_b_bw, perspectives,hidden_size):\n",
        "\n",
        "    fw_sim_matrix = cosine_matrix(sentence_a_fw,sentence_b_fw)\n",
        "    bw_sim_matrix = cosine_matrix(sentence_a_bw,sentence_b_bw)\n",
        "\n",
        "    #sentence_b_len = tf.reduce_sum(sentence_b_fw, 1)\n",
        "\n",
        "\n",
        "\n",
        "    # tf.get_variable_scope().reuse_variables()\n",
        "    with tf.variable_scope(\"Full_matching\",reuse=tf.AUTO_REUSE): \n",
        "\n",
        "\n",
        "      last_output_b_fw = get_last_output(sentence_b_fw)\n",
        "      \n",
        "      weights_fw = tf.get_variable(\"forward_matching\",\n",
        "        shape=[perspectives,hidden_size],\n",
        "        dtype=\"float\")\n",
        "\n",
        "      fw_full_match = full_matching(sentence_a_fw,last_output_b_fw,weights_fw)\n",
        "\n",
        "      last_output_b_bw = get_last_output(sentence_b_bw)\n",
        "\n",
        "      weights_bw = tf.get_variable(\"backward_matching\",\n",
        "        shape=[perspectives,hidden_size],\n",
        "        dtype=\"float\")\n",
        "      \n",
        "      #tf.get_variable_scope().reuse_variables()\n",
        "\n",
        "      bw_full_match = full_matching(sentence_a_bw,last_output_b_bw,weights_bw)\n",
        "\n",
        "\n",
        "    with tf.variable_scope(\"Pooling_matching\",reuse=tf.AUTO_REUSE): \n",
        "\n",
        "      \n",
        "      weights_fw = tf.get_variable(\"forward_matching\",\n",
        "        shape=[perspectives,hidden_size],\n",
        "        dtype=\"float\")\n",
        "\n",
        "      fw_pool_match = pooling_matching(sentence_a_fw,sentence_b_fw,weights_fw)\n",
        "\n",
        "      weights_bw = tf.get_variable(\"backward_matching\",\n",
        "        shape=[perspectives,hidden_size],\n",
        "        dtype=\"float\")\n",
        "      \n",
        "      #tf.get_variable_scope().reuse_variables()\n",
        "\n",
        "      bw_pool_match = pooling_matching(sentence_a_bw,sentence_b_bw,weights_bw)\n",
        "\n",
        "    with tf.variable_scope(\"Attentive_matching\",reuse=tf.AUTO_REUSE): \n",
        "\n",
        "\n",
        "      sentence_b_fw_att = weighted_sim(sentence_b_fw,fw_sim_matrix)\n",
        "\n",
        "\n",
        "      weights_fw = tf.get_variable(\"forward_matching\",\n",
        "        shape=[perspectives,hidden_size],\n",
        "        dtype=\"float\")\n",
        "\n",
        "      fw_att_match = attentive_matching(sentence_a_fw,sentence_b_fw_att,weights_fw)\n",
        "\n",
        "\n",
        "      sentence_b_bw_att = weighted_sim(sentence_b_bw,bw_sim_matrix)\n",
        "\n",
        "      weights_bw = tf.get_variable(\"backward_matching\",\n",
        "        shape=[perspectives,hidden_size],\n",
        "        dtype=\"float\")\n",
        "      \n",
        "      #tf.get_variable_scope().reuse_variables()\n",
        "\n",
        "      bw_att_match = attentive_matching(sentence_a_bw,sentence_b_bw_att,weights_bw)\n",
        "\n",
        "    with tf.variable_scope(\"Max_Attentive_matching\",reuse=tf.AUTO_REUSE): \n",
        "\n",
        "\n",
        "      sentence_b_fw_att_max = max_sim(sentence_b_fw,fw_sim_matrix)\n",
        "\n",
        "\n",
        "      weights_fw = tf.get_variable(\"forward_matching\",\n",
        "        shape=[perspectives,hidden_size],\n",
        "        dtype=\"float\")\n",
        "\n",
        "      fw_max_att_match = attentive_matching(sentence_a_fw,sentence_b_fw_att_max,weights_fw)\n",
        "\n",
        "\n",
        "      sentence_b_bw_att_max = max_sim(sentence_b_bw,bw_sim_matrix)\n",
        "\n",
        "      weights_bw = tf.get_variable(\"backward_matching\",\n",
        "        shape=[perspectives,hidden_size],\n",
        "        dtype=\"float\")\n",
        "      \n",
        "      #tf.get_variable_scope().reuse_variables()\n",
        "\n",
        "      bw_max_att_match = attentive_matching(sentence_a_bw,sentence_b_bw_att_max,weights_bw)\n",
        "\n",
        "    return [fw_full_match,bw_full_match,fw_pool_match,bw_pool_match,fw_att_match,bw_att_match,fw_max_att_match,bw_max_att_match]\n",
        "\n",
        "\n",
        "def bidirectional_match(sentence_a_fw,sentence_a_bw,sentence_b_fw,sentence_b_bw,perspectives=20,hidden_size=100):\n",
        "\n",
        "\n",
        "    match_1_2 = match(sentence_a_fw,sentence_a_bw,sentence_b_fw,sentence_b_bw,perspectives,hidden_size)\n",
        "    match_2_1 = match(sentence_b_fw,sentence_b_bw,sentence_a_fw,sentence_a_bw,perspectives,hidden_size)\n",
        "\n",
        "    match_1_2 = tf.concat(match_1_2,2)\n",
        "    match_2_1 = tf.concat(match_2_1,2)\n",
        "\n",
        "    return match_1_2,match_2_1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-9wkIrnu-O5",
        "outputId": "baf73412-a663-4698-eea1-c13261bcb7b8"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle as cPickle\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "# from matching import bidirectional_match,get_last_output\n",
        "# from data_loader import *\n",
        "tf.reset_default_graph()\n",
        "\n",
        "def log(message,file_path=os.path.join('/content/drive/MyDrive/ParaphraseDetection/glove_lstm','log.txt')):\n",
        "  print(message)\n",
        "  f1=open(file_path, 'a+')\n",
        "  f1.write(message)\n",
        "  f1.write(\"\\n\")\n",
        "  f1.close()\n",
        "\n",
        "def one_hot(batch_size,Y):\n",
        "\n",
        "  B = np.zeros((batch_size,2))\n",
        "\n",
        "  B[np.arange(batch_size),Y] = 1\n",
        "\n",
        "  return B.astype(int)\n",
        "\n",
        "def length(sequence):\n",
        "  used = tf.sign(sequence)\n",
        "  length = tf.reduce_sum(used, 1)\n",
        "  length = tf.cast(length, tf.int32)\n",
        "  \n",
        "  return length\n",
        "\n",
        "class model:\n",
        "\n",
        "  def __init__(self):\n",
        "\n",
        "    self.word2idx = {'PAD' : 0}\n",
        "    self.weights = []\n",
        "    \n",
        "    self.features = {}\n",
        "    self.word_list = []\n",
        "\n",
        "    self.glove_path = '/content/drive/MyDrive/ParaphraseDetection/glove.6B.200d.txt'\n",
        "    \n",
        "    self.dataset_path_test = '/content/drive/MyDrive/ParaphraseDetection/msr_paraphrase_test.txt'\n",
        "    self.dataset_path_train = '/content/drive/MyDrive/ParaphraseDetection/msr_paraphrase_train.txt'\n",
        "    \n",
        "\n",
        "    self.learning_rate = 0.0001\n",
        "  \n",
        "  def load_glove(self):\n",
        "\n",
        "    glove_cache_file = os.path.join('/content/drive/MyDrive/ParaphraseDetection/cache', 'glove.pkl')\n",
        "    word2idx_cache_file = os.path.join('/content/drive/MyDrive/ParaphraseDetection/cache', 'word2idx.pkl')\n",
        "    #if  path already present means glove embediings are already created so just reload it from memory.\n",
        "\n",
        "    if os.path.isfile(glove_cache_file):\n",
        "      print('Loading glove embeddings from : ' + glove_cache_file)\n",
        "      with open(glove_cache_file, 'rb') as f:\n",
        "        self.weights = cPickle.load(f)\n",
        "      print('Done')\n",
        "\n",
        "      print('Loading word2idx from : ' + word2idx_cache_file)\n",
        "      with open(word2idx_cache_file, 'rb') as f:\n",
        "        self.word2idx = cPickle.load(f)\n",
        "      print('Done')\n",
        "\n",
        "      self.embed_dim = len(self.weights[0])\n",
        "      self.vocab_size = self.weights.shape[0]\n",
        "      # self.word2idx['UNK'] = len(self.weights) - 1\n",
        "      print (self.weights.shape[0])#1,00,003\n",
        "      print (len(self.weights[0]))#300\n",
        "      print (len(self.weights))\n",
        "\n",
        "    else:\n",
        "            \n",
        "      print('Creating glove embeddings:')\n",
        "\n",
        "      with open(self.glove_path,'r') as file:\n",
        "\n",
        "        for index, line in enumerate(file):\n",
        "          values = line.split()\n",
        "          word = values[0]\n",
        "        \n",
        "          try:\n",
        "            word_weights = np.asarray(values[1:],dtype=np.float32)\n",
        "            if(word_weights.shape[0] == 200):\n",
        "              self.word2idx[word] = index+1\n",
        "              self.weights.append(word_weights)\n",
        "\n",
        "          except ValueError:\n",
        "            print('Error at line ',index)\n",
        "\n",
        "          if index == 100000:\n",
        "            break\n",
        "\n",
        "      self.embed_dim = len(self.weights[0])\n",
        "      self.weights.insert(0,np.random.randn(self.embed_dim))\n",
        "\n",
        "      self.word2idx['UNK'] = len(self.weights)\n",
        "      self.weights.append(np.random.randn(self.embed_dim))\n",
        "      \n",
        "      self.weights = np.stack(self.weights)\n",
        "      \n",
        "      self.vocab_size = self.weights.shape[0]\n",
        "\n",
        "      print('Saving word2idx to: ' + word2idx_cache_file)\n",
        "      with open(word2idx_cache_file, 'wb') as f:\n",
        "        cPickle.dump(self.word2idx,f)\n",
        "      print('Done!')\n",
        "\n",
        "      print('Saving glove embeddings to: ' + glove_cache_file)\n",
        "      with open(glove_cache_file, 'wb') as f:\n",
        "        cPickle.dump(self.weights,f)\n",
        "      print('Done!')\n",
        "          \n",
        "\n",
        "    print(self.vocab_size,self.embed_dim)\n",
        "\n",
        "    \n",
        "    print(\"Shape of glove embeddings:\",self.weights.shape)\n",
        "    # print(self.weights)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  #os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
        "  x = model()\n",
        "\n",
        "  x.sen_len = 41\n",
        "\n",
        "  x.load_glove()\n",
        "  \n",
        "\n",
        "  x.sentence_one,x.sentence_two,x.y_true = load_dataset(x.dataset_path_train,x.word2idx)\n",
        "  print( x.sentence_one)\n",
        "\n",
        "  x.sentence_one_test,x.sentence_two_test,x.y_true_test = load_dataset_test(x.dataset_path_test,x.word2idx)\n",
        "  # exit(0)\n",
        "  dropout_rate = 0.1\n",
        "  num_epoch = 20\n",
        "  batch_size = 256\n",
        "  print( x.sentence_one_test)\n",
        "\n",
        "  X_init = tf.placeholder(tf.float32, shape=(x.vocab_size,x.embed_dim))\n",
        "  print(x.vocab_size)\n",
        "  print(x.embed_dim)\n",
        "\n",
        "  sentence_one = tf.placeholder(\"int32\",[batch_size, x.sen_len],name=\"sentence_one\")\n",
        "  sentence_two = tf.placeholder(\"int32\",[batch_size, x.sen_len],name=\"sentence_two\")\n",
        "\n",
        "  Y = tf.placeholder(\"int32\",[batch_size, 2],name=\"true_labels\")\n",
        "  \n",
        "  embedding_weights = tf.get_variable(name = 'embedding_weights', initializer = X_init, trainable = False)\n",
        "\n",
        "\n",
        "  # print sentence_one_len\n",
        "\n",
        "  with tf.name_scope(\"Word_embeddings\"):\n",
        "\n",
        "    embedded_sentence_one = tf.nn.embedding_lookup(embedding_weights,sentence_one)\n",
        "    embedded_sentence_two = tf.nn.embedding_lookup(embedding_weights,sentence_two)\n",
        "\n",
        "\n",
        "  with tf.variable_scope(\"Context_representation_layer\"):\n",
        "\n",
        "    context_rnn_hidden_size = 100\n",
        "\n",
        "    sentence_enc_fw = tf.nn.rnn_cell.LSTMCell(context_rnn_hidden_size,state_is_tuple=True)\n",
        "    sentence_enc_bw = tf.nn.rnn_cell.LSTMCell(context_rnn_hidden_size,state_is_tuple=True)\n",
        "\n",
        "    # Sentence 1\n",
        "    # Shape (batch_size, sequence_length, rnn_hidden_size)\n",
        "\n",
        "    outputs_1, states_1  = tf.nn.bidirectional_dynamic_rnn(\n",
        "        cell_fw=sentence_enc_fw,\n",
        "        cell_bw=sentence_enc_bw,\n",
        "        dtype=tf.float32,\n",
        "        # sequence_length=length(sentence_one),\n",
        "        inputs=embedded_sentence_one)\n",
        "\n",
        "    output_fw_1, output_bw_1 = outputs_1\n",
        "    states_fw_1, states_bw_1 = states_1\n",
        "\n",
        "    output_fw_1 = tf.layers.dropout(\n",
        "      output_fw_1,\n",
        "      rate=dropout_rate,\n",
        "      training=False,\n",
        "      name=\"sentence_1_fw_dropout\")\n",
        "    \n",
        "    output_bw_1 = tf.layers.dropout(\n",
        "      output_bw_1,\n",
        "      rate=dropout_rate,\n",
        "      training=False,\n",
        "      name=\"sentence_1_bw_dropout\")\n",
        "\n",
        "    tf.get_variable_scope().reuse_variables()\n",
        "\n",
        "    #Sentence 2\n",
        "\n",
        "    # Shape (batch_size, sequence_length, rnn_hidden_size)\n",
        "\n",
        "    outputs_2, states_2  = tf.nn.bidirectional_dynamic_rnn(\n",
        "        cell_fw=sentence_enc_fw,\n",
        "        cell_bw=sentence_enc_bw,\n",
        "        dtype=tf.float32,\n",
        "        # sequence_length=length(sentence_two),\n",
        "        inputs=embedded_sentence_two)\n",
        "    \n",
        "    output_fw_2, output_bw_2 = outputs_2\n",
        "    states_fw_2, states_bw_2 = states_2\n",
        "\n",
        "    output_fw_2 = tf.layers.dropout(\n",
        "      output_fw_2,\n",
        "      rate=dropout_rate,\n",
        "      training=False,\n",
        "      name=\"sentence_2_fw_dropout\")\n",
        "    output_bw_2 = tf.layers.dropout(\n",
        "      output_bw_2,\n",
        "      rate=dropout_rate,\n",
        "      training=False,\n",
        "      name=\"sentence_2_bw_dropout\")\n",
        "\n",
        "\n",
        "  #tf.get_variable_scope().reuse_variables()\n",
        "  with tf.variable_scope(\"Matching_layer\"):\n",
        "    \n",
        "    # Shape (batch_size, sequence_length, 8*perspectives)\n",
        "\n",
        "\n",
        "    match_1_2,match_2_1 = bidirectional_match(output_fw_1,output_bw_1,output_fw_2,output_bw_2)\n",
        "\n",
        "    match_1_2 = tf.layers.dropout(\n",
        "      match_1_2,\n",
        "      rate=dropout_rate,\n",
        "      training=False,\n",
        "      name=\"match_1_2_dropout\")\n",
        "    match_2_1 = tf.layers.dropout(\n",
        "      match_2_1,\n",
        "      rate=dropout_rate,\n",
        "      training=False,\n",
        "      name=\"match_2_1_dropout\")\n",
        "\n",
        "\n",
        "  ## Aggregation Layer\n",
        "\n",
        "  with tf.variable_scope(\"Aggregation_layer\"):\n",
        "\n",
        "    agg_last = []\n",
        "\n",
        "    # Sentence 1\n",
        "\n",
        "    with tf.variable_scope(\"agg_1\",reuse=tf.AUTO_REUSE):\n",
        "\n",
        "      aggregation_enc_fw = tf.nn.rnn_cell.LSTMCell(context_rnn_hidden_size,state_is_tuple=True)\n",
        "      aggregation_enc_bw = tf.nn.rnn_cell.LSTMCell(context_rnn_hidden_size,state_is_tuple=True)\t\n",
        "\n",
        "      # Shape (batch_size, sequence_length, rnn_hidden_size)\n",
        "\n",
        "\n",
        "      agg_outputs_1, agg_states_1  = tf.nn.bidirectional_dynamic_rnn(\n",
        "          cell_fw=aggregation_enc_fw,\n",
        "          cell_bw=aggregation_enc_bw,\n",
        "          dtype=tf.float32,\n",
        "          # sequence_length=length(sentence_one),\n",
        "          inputs=match_1_2)\n",
        "\n",
        "      agg_output_fw_1, agg_output_bw_1 = agg_outputs_1\n",
        "\n",
        "      agg_output_fw_1 = tf.layers.dropout(\n",
        "        agg_output_fw_1,\n",
        "        rate=dropout_rate,\n",
        "        training=False,\n",
        "        name=\"agg_1_fw_dropout\")\n",
        "      agg_output_bw_1 = tf.layers.dropout(\n",
        "        agg_output_bw_1,\n",
        "        rate=dropout_rate,\n",
        "        training=False,\n",
        "        name=\"agg_1_bw_dropout\")\n",
        "\n",
        "      # Shape : (batch_size, rnn_hidden_size)\n",
        "\n",
        "      agg_output_fw_1_last = get_last_output(agg_output_fw_1)\n",
        "      agg_output_bw_1_last = get_last_output(agg_output_bw_1)\n",
        "\n",
        "      agg_last.append(agg_output_fw_1_last)\n",
        "      agg_last.append(agg_output_bw_1_last)\n",
        "\n",
        "    #tf.get_variable_scope().reuse_variables()\n",
        "\n",
        "    #Sentence 2\n",
        "\n",
        "    with tf.variable_scope(\"agg_2\",reuse=tf.AUTO_REUSE):\n",
        "\n",
        "\n",
        "      aggregation_enc_fw_2 = tf.nn.rnn_cell.LSTMCell(context_rnn_hidden_size,state_is_tuple=True)\n",
        "      aggregation_enc_bw_2 = tf.nn.rnn_cell.LSTMCell(context_rnn_hidden_size,state_is_tuple=True)\n",
        "\n",
        "      # Shape (batch_size, sequence_length, rnn_hidden_size)\n",
        "\n",
        "      agg_outputs_2, agg_states_2  = tf.nn.bidirectional_dynamic_rnn(\n",
        "          cell_fw=aggregation_enc_fw_2,\n",
        "          cell_bw=aggregation_enc_bw_2,\n",
        "          dtype=tf.float32,\n",
        "          # sequence_length=length(sentence_two),\n",
        "          inputs=match_2_1)\n",
        "      \n",
        "      agg_output_fw_2, agg_output_bw_2 = agg_outputs_2\n",
        "\n",
        "      agg_output_fw_2 = tf.layers.dropout(\n",
        "        agg_output_fw_2,\n",
        "        rate=dropout_rate,\n",
        "        training=False,\n",
        "        name=\"agg_2_fw_dropout\")\n",
        "      agg_output_bw_2 = tf.layers.dropout(\n",
        "        agg_output_bw_2,\n",
        "        rate=dropout_rate,\n",
        "        training=False,\n",
        "        name=\"agg_2_bw_dropout\")\n",
        "\n",
        "\n",
        "      # Shape : (batch_size, rnn_hidden_size)\n",
        "\n",
        "      agg_output_fw_2_last = get_last_output(agg_output_fw_2)\n",
        "      agg_output_bw_2_last = get_last_output(agg_output_bw_2)\n",
        "\n",
        "\n",
        "      agg_last.append(agg_output_fw_2_last)\n",
        "      agg_last.append(agg_output_bw_2_last)\n",
        "\n",
        "    # Shape : (batch_size, 4*rnn_hidden_size)\n",
        "\n",
        "    combined_agg_last = tf.concat(agg_last,1)\n",
        "\n",
        "\n",
        "  ##Prediction Layer\n",
        "  with tf.variable_scope(\"Prediction_layer\",reuse=tf.AUTO_REUSE):\n",
        "\n",
        "    prediction_layer_1 = tf.layers.dense(combined_agg_last,\n",
        "      combined_agg_last.get_shape().as_list()[1],\n",
        "      activation=tf.nn.tanh,\n",
        "      name=\"pred_1\")\n",
        "    \n",
        "    prediction_layer_1 = tf.layers.dropout(\n",
        "      prediction_layer_1,\n",
        "      rate=dropout_rate,\n",
        "      training=False,\n",
        "      name=\"pred_dropout\")\n",
        "\n",
        "    prediction_layer_2 = tf.layers.dense(prediction_layer_1,2,name=\"pred_2\")\n",
        "\n",
        "  with tf.variable_scope(\"Train_loss_and_acc\"):\n",
        "\n",
        "    y_pred = tf.nn.softmax(prediction_layer_2)\n",
        "    print(y_pred)\n",
        "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y,logits=prediction_layer_2))\n",
        "    \n",
        "    tf.summary.scalar('loss',loss_op)\t\t\n",
        "\n",
        "    correct_predictions = tf.equal(tf.argmax(y_pred, 1),tf.argmax(Y, 1))\n",
        "\n",
        "    batch_accuracy = tf.reduce_mean(tf.cast(correct_predictions,\"float\"))\n",
        "    tf.summary.scalar('accuracy',batch_accuracy)\n",
        "\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=x.learning_rate)\n",
        "    train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "  merged = tf.summary.merge_all()\n",
        "  saver = tf.train.Saver()\n",
        "\n",
        "\n",
        "  with tf.Session() as sess:\n",
        "\n",
        "    trained_model = os.path.join('/content/drive/MyDrive/ParaphraseDetection/glove_lstm', 'model.ckpt')\n",
        "    trained_model_restore = os.path.join('/content/drive/MyDrive/ParaphraseDetection/glove_lstm', 'model.ckpt.meta')\n",
        "\n",
        "    if os.path.isfile(trained_model_restore):\n",
        "\n",
        "      print(\"Loading saved model...\")\n",
        "      saver.restore(sess, trained_model)\n",
        "\n",
        "    else:\n",
        "      train_writer = tf.summary.FileWriter(\"./loga/1/train\",sess.graph)\n",
        "      print(\"No saved model found...Training...\")\n",
        "      sess.run(tf.global_variables_initializer(),feed_dict={X_init:x.weights})\n",
        "\n",
        "      i = 0.0\n",
        "      sum_acc = 0.0\n",
        "\n",
        "      for epoch in range(num_epoch):\n",
        "\n",
        "        for step in range(int(x.sentence_one.shape[0]/batch_size)):\n",
        "\n",
        "          i += 1\n",
        "\n",
        "          batch_s1,batch_s2,batch_y = x.sentence_one[step*batch_size:(step+1)*batch_size],\\\n",
        "                        x.sentence_two[step*batch_size:(step+1)*batch_size],\\\n",
        "                        x.y_true[step*batch_size:(step+1)*batch_size]\n",
        "\n",
        "          #print batch_s1.shape,batch_s2.shape,batch_y.shape\n",
        "\n",
        "          batch_y = one_hot(batch_size,batch_y.astype(int))\n",
        "\n",
        "          [summary,_,loss,acc] = sess.run([merged,train_op,loss_op,batch_accuracy],\n",
        "                  feed_dict={X_init:x.weights,\n",
        "                  sentence_one: batch_s1,\n",
        "                  sentence_two: batch_s2,\n",
        "                  Y:batch_y\n",
        "                  })\n",
        "\n",
        "          train_writer.add_summary(summary, i)\n",
        "\n",
        "\n",
        "          sum_acc += acc\n",
        "\n",
        "          log(\"Epoch:\" + str(epoch+1) + \" Step:\" + str(step) + \" Loss:\" + \"{:.4f}\".format(loss) + \" Batch Acc:\" + \"{:.4f}\".format(acc) + \" Mean Batch Acc:\" + \"{:.4f}\".format(sum_acc/i))\n",
        "    \n",
        "      save_path = saver.save(sess, trained_model)\n",
        "      print(\"Model saved in path: %s\" % save_path)\n",
        "\n",
        "    print(\"Testing Model...\")\n",
        "    i = 0\n",
        "    sum_acc = 0.0\n",
        "    for step in range(int(x.sentence_one_test.shape[0]/batch_size)):\n",
        "\n",
        "      i += 1\n",
        "\n",
        "      batch_s1,batch_s2,batch_y = x.sentence_one_test[step*batch_size:(step+1)*batch_size],\\\n",
        "                    x.sentence_two_test[step*batch_size:(step+1)*batch_size],\\\n",
        "                    x.y_true_test[step*batch_size:(step+1)*batch_size]\n",
        "\n",
        "      batch_y = one_hot(batch_size,batch_y.astype(int))  \n",
        "  \n",
        "      [acc] = sess.run([batch_accuracy],\n",
        "              feed_dict={X_init:x.weights,\n",
        "              sentence_one: batch_s1,\n",
        "              sentence_two: batch_s2,\n",
        "              Y:batch_y\n",
        "              })\n",
        "      sum_acc += acc\n",
        "\n",
        "      log(\"Batch Test Accuracy:\" + \"{:.4f}\".format(acc) + \" Mean Batch Test Acc:\" + \"{:.4f}\".format(sum_acc/i))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Loading glove embeddings from : /content/drive/MyDrive/ParaphraseDetection/cache/glove.pkl\n",
            "Done\n",
            "Loading word2idx from : /content/drive/MyDrive/ParaphraseDetection/cache/word2idx.pkl\n",
            "Done\n",
            "100003\n",
            "200\n",
            "100003\n",
            "100003 200\n",
            "Shape of glove embeddings: (100003, 200)\n",
            "Max_train: 38 41\n",
            "(4075, 41) (4075, 41) (4075,)\n",
            "[[ 85049   1179  40720 ...      0      0      0]\n",
            " [    40     41    736 ...      0      0      0]\n",
            " [   205 100002   1772 ...      0      0      0]\n",
            " ...\n",
            " [    29     54     34 ...      0      0      0]\n",
            " [     1  14675     16 ...      0      0      0]\n",
            " [     1   5794   1847 ...      0      0      0]]\n",
            "Max_test: 36 35\n",
            "(1724, 41) (1724, 41) (1724,)\n",
            "[[     1     86     10 ...      0      0      0]\n",
            " [   201      5      1 ...      0      0      0]\n",
            " [     8   3526   1837 ...      0      0      0]\n",
            " ...\n",
            " [ 33531   3960   1044 ...      0      0      0]\n",
            " [    77    148      1 ...      0      0      0]\n",
            " [100002  12714     62 ...      0      0      0]]\n",
            "100003\n",
            "200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:903: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  warnings.warn(\"`tf.nn.rnn_cell.LSTMCell` is deprecated and will be \"\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1727: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/legacy_tf_layers/core.py:268: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
            "  warnings.warn('`tf.layers.dropout` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1719: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/legacy_tf_layers/core.py:171: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  warnings.warn('`tf.layers.dense` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"Train_loss_and_acc/Softmax:0\", shape=(256, 2), dtype=float32)\n",
            "Loading saved model...\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/ParaphraseDetection/glove_lstm/model.ckpt\n",
            "Testing Model...\n",
            "Batch Test Accuracy:0.7266 Mean Batch Test Acc:0.7266\n",
            "Batch Test Accuracy:0.7500 Mean Batch Test Acc:0.7383\n",
            "Batch Test Accuracy:0.7188 Mean Batch Test Acc:0.7318\n",
            "Batch Test Accuracy:0.7109 Mean Batch Test Acc:0.7266\n",
            "Batch Test Accuracy:0.7227 Mean Batch Test Acc:0.7258\n",
            "Batch Test Accuracy:0.7070 Mean Batch Test Acc:0.7227\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjGd7mqi6rr8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}